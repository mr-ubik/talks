<!-- sectionTitle: TF Episode 2.0: TF Strikes Back -->
<!-- classes: tf2-bg -->

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: You either die a static graph or live long enough to become Eager by default -->

<header>
    <p class="aligncenter">Eager to try Eager Mode? ü§î </p>
</header>

## You either die a static graph or live long enough to become Eager by default

<hr />

<blockquote class="text-quote size-50 aligncenter">
TensorFlow‚Äôs eager execution is an imperative programming environment that evaluates
operations immediately, without building graphs: operations return concrete values
instead of constructing a computational graph to run later.
</blockquote>

<hr />

<blockquote class="text-quote size-70 aligncenter">
TensorFlow 1.X requires users to manually stitch together an abstract syntax tree (the graph)
by making tf.* API calls. It then requires users to manually compile the abstract syntax tree by
passing a set of output tensors and input tensors to a session.run() call.<br />
TensorFlow 2.0 executes eagerly (like Python normally does) and in 2.0, graphs and sessions should feel
like implementation details.<br />
One notable byproduct of eager execution is that <code>tf.control_dependencies()</code> is no longer
required, as all lines of code execute in order (within a <code>tf.function</code>, code with side
effects execute in the order written).
</blockquote>
<img class="aligncenter" src="../images/eager.gif" alt="Eager in the sign language" />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

## üéâ Yes, we can now debug and write Python code (especially control flow) as in vanilla Python. üéâ

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: Goodbye cruel Globals -->
## ‚ôªÔ∏è Hasta la vista globals ‚ôªÔ∏è

<p class="size-50 aligncenter">
    In a decision of environmental responsability, the TensorFlow team, according to the
    <a href="https://github.com/tensorflow/community/pull/11"> Variables 2.0 RFC</a> decided
    it was time to let orphaned global variables be handled by the Garbage Collector.<br />
    So you either keep track of your Python pointers or you lose your variables.<br />
    While this may seem to create additional overhead for the end-user, together with Keras
    objects that beahave like sane Python objects (and handle this tracking for you) it actually
    simplify code structure and usability by quite a lot.
</p>

<img class="aligncenter" src="../images/hlv.gif" alt="Terminator: Hasta la vista baby" />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------
<!-- sectionTitle: Fun, Fun, Functions (bye bye Sexy, Sexy, Session) -->
<!-- classes: peace-bg -->

<h3 class="aligncenter bg-trans-dark">‚òÆÔ∏è Make ~~love~~ Functions Not ~~war~~ Sessions ‚òÆÔ∏è</h3>

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter">Fun, Fun, Functions</p>
</header>

<p class="aligncenter size-70">
TensorFlow has gone from a Pure Static Graph to Eager by default while keeping the power and versatilty
of Graph compilation by using the <code>tf.function</code> decorator.
This feature is built on top of the <a hfre="https://www.tensorflow.org/beta/guide/autograph">TF Autograph</a>,
which let users define static graph using a natural, familiar, Python syntax.
Using the <code>tf.function</code> on a Python Function will convert the code from the pure vanilla Python
syntax, to a static Graph.
Gone is the need of using TensorFlow control flows primitives, you can write Python control
flow and have it converted to highly performant static graph code.
<hr />
The magic goes deeper but to cover it you will need a dedicated talk.<br />
Luckily you can delve into the mysteries of <b>Autograph</b> later with <br />
<a href="https://ep2019.europython.eu/talks/vDUYo7h-dissecting-tffunction-to-discover-autograph-strengths-and-subtleties/">
Paolo Galeone's talk: Dissecting tf.function to discover AutoGraph strengths and subtleties</a>
</p>

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: The TF 1.X API is dead. Long Live Keras -->
<!-- classes: dead-kermit-bg -->

<h3 class="aligncenter bg-trans-dark">The Death of an API</h3>

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

----

<!-- classes: panic -->

### `tf.layers` is dead

### `tf.Graph` is gone (into hiding)

### `tf.contrib` is dead

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<!-- classes: bg-celebration -->

# üéâ  It was about time  üéâ

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<!-- classes: keras -->

### Long Live Keras

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: All the API you want as long as it is Keras -->

<div class="grid">
    <div class="column">
        <ul class="flexblock features">
            <li><h4>High Level API Spec with usability in mind</h4></li>
            <li><h4>Modular backend (TensorFlow üíñ, CNTK üíÄ, Theano üíÄ)</h4></li>
            <li><h4>Keras Layers and Models behave in a sane and Pythonic way</h4></li>
            <li><h4>Chainer API for available for the expert practitioner</h4></li>
            <li><h4><a href="https://keras.io">Keras Website</a></h4></li>
            <li><h4><a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras">tf.keras API on TensorFlow Docs</a></h4></li>
        </ul>
    </div>
</div>

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Layers</p>
</header>

### Keras Layers

* One and only Layer API
* Available under `tensorflow.keras.layers`
* Pythonic Object
* Simple to use:
    1. Initialize the Layer
    2. Use layers as callable

```python
awesome_layer = keras.layers.Dense(
    units=50,
    activation="tanh",
)
awesome_layer(awesome_inputs)
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Losses & Optimizers</p>
</header>

### Keras Losses

Losses now are contained it the `tf.keras.losses` module.

While TF 2.0 offers an ample out of the box selections of them, you can easily define<br />
custom ones by subclassing from `tf.keras.losses.Loss`, the only strict requirement is the<br />
implementation of a `call()` method accepting `y_true` and `y_pred` as arguments.

<hr />

### Keras Optimizers

Gone are the old TF optimizers, now they live under the `tf.keras.optimizers` module.<br />
We will see later how they are used during the training step.

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: Defining Models with Keras -->

### Keras Model API

#### **> Sequential**

#### Functional

#### Subclassing (Chainer) API

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Sequential Model</p>
</header>

### Keras Sequential Model

The sequential model is the most straightforward of the the three APIs,<br />
it consists of a stacked sequence of layers, each feeding into the next in a linear graph.

<hr />

It can be constructed either by passing a list of `keras.layers` at its instantiation <br /> or built by
iteratively calling its `add()` method and passing the desired layer.

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Sequential Model</p>
</header>

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(...),
    tf.keras.layers.BatchNormalization(...),
    tf.keras.layers.Dense(...),
    tf.keras.layers.Softmax(...)
])
```

```python
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(...))
model.add(tf.keras.layers.BatchNormalization(...))
model.add(tf.keras.layers.Dense(...))
model.add(tf.keras.layers.Softmax(...))
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

## Keras Model API

### Sequential

### **> Functional **

### Subclassing (Chainer) API

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Model Functional API</p>
</header>

### Keras Model Functional API

While `keras.Sequential` is enough to cover a vast portion of use-cases, there are times<br />
where a linear stack of layers cannot simply represent a more complex architecture.

Keras Functional API comes to the rescue by offering us control over how and when are layers
instantiated and called.
<hr />
<blockquote class="text-quote size-70 aligncenter">
<em>[The Functional API]</em> can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs.<br />
It's based on the idea that a deep learning model is usually a directed acyclic graph (DAG) of layers.
The Functional API a set of tools for <b>building graphs of layers</b>.
</blockquote>
<hr />

For more information see: [The Keras Functional API in TensorFlow](https://www.tensorflow.org/beta/guide/keras/functional)

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Model Functional API</p>
</header>

1) Start by creating an Input Node. **NOTE:** we never specify the batch size.<br />
What gets returned, `inputs`, contains information about the shape and dtype of<br />
the input data that you expect to feed to your model:

```python
inputs = tf.keras.Input(shape=(784,))
```

2) To add nodes to the graph simply call a `tf.keras.Layer` or `tf.keras.Model` on the `inputs`<br />
**NOTE:** `Layer` and `Model` need to be initialized before the the `__call__()` method is exposed.

```python
inputs = tf.keras.Input(shape=(784,), name="img")
x = tf.keras.layers.Dense(64, activation="relu")(inputs)
x = tf.keras.layers.Dense(64, activation="relu")(x)
outputs = tf.keras.layers.Dense(10, activation="softmax")(x)
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Model Functional API</p>
</header>

3) We can create a `Model` by specifying its inputs and outputs in the graph of layers.<br />
**NOTE:** In case of a `Model` with multiple inputs/outputs, we just need to pass them as `list`.

```python
model = keras.Model(inputs=inputs, outputs=outputs)
```

4) After that our model can be inspected with `model.summary()`

```bash
Model: "mnist_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
img (InputLayer)             [(None, 784)]             0
_________________________________________________________________
dense_3 (Dense)              (None, 64)                50240
_________________________________________________________________
dense_4 (Dense)              (None, 64)                4160
_________________________________________________________________
dense_5 (Dense)              (None, 10)                650
=================================================================
Total params: 55,050
Trainable params: 55,050
Non-trainable params: 0
_________________________________________________________________
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

## Keras Model API

### Sequential

### Functional

### **> Subclassing (Chainer) API**

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Model Subclassing API</p>
</header>

### Keras Model Subclassing (Chainer) API

Made popular by the Chainer Deep Learning Framework, this is regarded as the API for the
1% researcher.<br />
The idea is simple, by subclassing from the `tf.keras.Model` object we can define our own
forward pass.<br />
Create layers in the `__init__` method and set them as attributes of the class instance.
Define the forward pass in the `call` method.<br />
**NOTE:** Model subclassing is particularly useful when eager execution is enabled,<br />
because it allows the forward pass to be written imperatively.<br />

<h3 class="aligncenter bg-white size-50">
<b class="bg-red">CAVEAT USER</b><br />
While this way of building model is extremely powerful
it comes at higher cost in terms of bug-likelihood and technical debt.</h3>

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Keras Model Subclassing API</p>
</header>

The following example shows a subclassed tf.keras.Model using a custom forward pass that
does not have to be run imperatively.

```python
class MyModel(tf.keras.Model):

    def __init__(self, num_classes=10):
        super(MyModel, self).__init__(name="my_model")
        self.num_classes = num_classes
        # Define your layers here.
        self.dense_1 = layers.Dense(32, activation="relu")
        self.dense_2 = layers.Dense(num_classes, activation="sigmoid")

    def call(self, inputs):
        # Define your forward pass here,
        # using layers you previously defined (in `__init__`).
        x = self.dense_1(inputs)
        return self.dense_2(x)
```

**Note:** to make sure the forward pass is always run imperatively,
you must set `dynamic=True` when calling the super constructor.

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: High Performance Input Pipeline with tf.data -->
<!-- classes: pipeline-bg -->

<h3 class="aligncenter bg-trans-dark">High Performance Input Pipeline with `tf.data`</h3>

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">Input Pipeline with <code>tf.data</code></p>
</header>

### `tf.data`

TensorFlow 1.X had support for highly performant input pipelines in the form of the `tf.data`
module. With 2.0 the module is still there and possibly even easier to use than before.
<hr />
<blockquote class="text-quote aligncenter size-70">
The <code>tf.data</code> API introduces a <code>tf.data.Dataset</code> abstraction
that represents a sequence of elements, in which each element consists of one or more Tensor objects.
For example, in an image pipeline, an element might be a single training example,
with a pair of tensors representing the image and its label.
<br />

<br />
There are two distinct ways to create a dataset:<br />
<br />
    <ul class="aligncenter size-70">
        <li>A data source constructs a Dataset from data stored in memory in one ore more files.</li>
        <li>A data transformation constructs a dataset from one or more tf.data.Dataset objects.</li>
    </ul>
</blockquote>

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter"></p>
</header>

### Plumbing

* In memory data: `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.
* Input data steored in the recommended TFRecord format: `tf.data.TFRecordDataset()`

Once you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the `tf.data.Dataset` object.<br />
For example, you can apply per-element transformations such as `Dataset.map()`, and multi-element transformations such as `Dataset.batch()`.<br />

See the [documentation for `tf.data.Dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset) for a complete list of transformations.

**NOTE:** In TF 2.0 the Dataset object is a Python iterable. This makes it possible to consume its elements using a for loop <br />;
another option is creating an iterable using Python `iter()` and consuming it with the the `next()` function.

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: Model Training -->
<!-- classes: rocky-training-bg -->

<h1 class="aligncenter bg-trans-dark">Model Training</h1>

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter"><code>model.compile()</code></p>
</header>

#### `model.compile()`

After having created your model with one of the above API,<br />
the last step is to let Keras bundle Model, Loss and Optimizer together.<br />
This bit of magic is done via the `tf.keras.Model.compile()` method which once called, configures the model for training.

<br />

`tf.keras.Model.compile` takes three important arguments:

* **`optimizer`**: This object specifies the training procedure.<br />
    Pass it optimizer instances from the `tf.keras.optimizers` module, such as `tf.keras.optimizers.Adam`<br />
    or `tf.keras.optimizers.SGD`. If you just want to use the default parameters,<br />
    you can also specify optimizers via strings, such as 'adam' or 'sgd'.<br />
* **`loss`**: The function to minimize during optimization. Common choices include mean square error (mse),<br />
    categorical_crossentropy, and binary_crossentropy. Loss functions are specified by name<br />
    or by passing a callable object from the `tf.keras.losses` module.<br />
* **`metrics`**: Used to monitor training. These are string names<br />
    or callables from the `tf.keras.metrics` module.<br />
* Additionally, to make sure the model trains and evaluates eagerly,<br />
    you can make sure to pass `run_eagerly=True` as a parameter to compile.

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter"><code>model.compile()</code></p>
</header>

```python
model = tf.keras.Sequential(
    [
        # Adds a densely-connected layer with 64 units to the model:
        tf.keras.layers.Dense(
            64, activation="relu", input_shape=(32,)
        ),
        # Add another:
        tf.keras.layers.Dense(64, activation="relu"),
        # Add a softmax layer with 10 output units:
        tf.keras.layers.Dense(10, activation="softmax"),
    ]
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter">Do you even <code>model.fit?</code>üí™</p>
</header>

## `model.fit()`

<img class="aligncenter" src="../images/modelfit.gif" alt="Terminator: Hasta la vista baby" />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter">model.fit()</p>
</header>

Unless you are in a particualr use case, `Model.fit` is the vastly preferred way to train Keras Models.<br />
It is fast, optimized and reliable.

`tf.keras.Model.fit()` has many optional arguments but these these 5 are the most important ones:
* **`x`:** Input data
* **`y`:** Target data
* **`epochs`:** Training is structured into epochs.<br />
    An epoch is one iteration over the entire input data (this is done in smaller batches).<br />
* **`batch_size`:** When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training.<br />
    This integer specifies the size of each batch. <br />
    Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size. <br />
* **`validation_data`:** When prototyping a model, you want to easily monitor its performance on some validation data.<br />
    Passing this argument‚Äîa tuple of inputs and labels‚Äîallows the model to display<br />
    the loss and metrics in inference mode for the passed data, at the end of each epoch.<br />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter">model.fit()</p>
</header>

```python
import numpy as np

data = np.random.random((1000, 32))
labels = random_one_hot_labels((1000, 10))

val_data = np.random.random((100, 32))
val_labels = random_one_hot_labels((100, 10))

model.fit(
    data,
    labels,
    epochs=10,
    batch_size=32,
    validation_data=(val_data, val_labels),
)
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter"><blockquote class="text-quote">I train my model a quarter of a tf.data.Dataset at a time üèé</blockquote></p>
</header>

### `Model.fit()` & `tf.data.Dataset`

Here, the fit method uses the `steps_per_epoch` argument‚Äîthis is the number of training steps<br />
the model runs before it moves to the next epoch.<br />
Since the Dataset yields batches of data, this snippet does not require a `batch_size`.<br />

`tf.data.Dataset` can also be used for validation:

```python
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset = dataset.batch(32).repeat()

val_dataset = tf.data.Dataset.from_tensor_slices(
    (val_data, val_labels)
)
val_dataset = val_dataset.batch(32).repeat()

model.fit(
    dataset,
    epochs=10,
    steps_per_epoch=30,
    validation_data=val_dataset,
    validation_steps=3,
)
```

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<header>
    <p class="aligncenter">DIY Calisthenics? ü§î</p>
</header>

## The Dark Powers: Custom Training

<img class="aligncenter" src="../images/necro.gif" alt="Terminator: Hasta la vista baby" />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

<header>
    <p class="aligncenter"><blockquote class="text-quote">‚ö°Ô∏è UNLIMITED POWER ‚ö°Ô∏è</blockquote></p>
</header>

_Beyond the safety of `model.fit()` and `model.evaluate()` lies the dark power of the `GradientTape`,<br />
those power-mad practitioners who embrace its powers trade their sanity in exchange for a perfect control over the training.<br />_

<hr />

Ominous description aside, this a powerful technique that can make the training of<br />
complex architecture more feasible at the cost of more possible errors.

<hr />

Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of <br />
the trainable weights of the layer with respect to a loss value. <br />
Using a `tf.keras.optimizer` instance, you can use these gradients to update these <br />
variables (which you can retrieve using `model.trainable_weights`).<br />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

---

```python
for epoch in range(3):
    print("Start of epoch %d" % (epoch,))

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(
        train_dataset
    ):

        # Open a GradientTape to record the operations run
        # during the forward pass, which enables autodifferentiation.
        with tf.GradientTape() as tape:

            # Run the forward pass of the layer.
            # The operations that the layer applies
            # to its inputs are going to be recorded
            # on the GradientTape.
            logits = model(
                x_batch_train
            )  # Logits for this minibatch

            # Compute the loss value for this minibatch.
            loss_value = loss_fn(y_batch_train, logits)

        # Use the gradient tape to automatically retrieve
        # the gradients of the trainable variables with respect to the loss.
        grads = tape.gradient(
            loss_value, model.trainable_weights
        )

        # Run one step of gradient descent by updating
        # the value of the variables to minimize the loss.
        optimizer.apply_gradients(
            zip(grads, model.trainable_weights)
        )

        # Log every 200 batches.
        if step % 200 == 0:
            print(
                "Training loss (for one batch) at step %s: %s"
                % (step, float(loss_value))
            )
            print(
                "Seen so far: %s samples"
                % ((step + 1) * 64)
            )
```

<footer>
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

<!-- sectionTitle: Exporting Models -->

<header>
    <p class="aligncenter">Exporting model</p>
</header>

## Exporting Models

[ALL YOU NEED TO KNOW ABOUT EXPORTING MODELS WE ARE RUNNING OUT OF TIME](https://www.tensorflow.org/beta/guide/keras/saving_and_serializing)

[ALL YOU NEED TO KNOW ABOUT SAVING SUBCLASSED MODELS](https://www.tensorflow.org/beta/guide/keras/saving_and_serializing#saving_subclassed_models)

<hr />

### TL;DR

Exporting is dead easy: `model.save` for Sequential and Functional API <br />
* ith four possible choices:
* Whole Model Saving
* Export whole model as a SavedModel
* Architecture-only-saving
* Weights-only-saving
* Weights-only-saving in SavedModel Format

<img class="aligncenter" src="../images/export.gif" alt="Terminator: Hasta la vista baby" />

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>

-----------------------------------------------------------------------------------------

# To Conclude

#### If you are using PyTorch --> Try TF 2.0

#### If you love Keras --> Try TF 2.0

#### If TF 1.X scared you away (for good reason) --> Try TF 2.0

<hr />

<em class="aligncenter"> ... Most importantly ... </em>

<hr />

#### Follow me on TwittaH --> @mr_ubik

#### Thank You All, may the Tensor be with you üíñ

#### HAPPY EUROPYTHON!

<footer class="bg-white">
    <a class="alignright" href="https://twitter.com/mr_ubik"><i class="fab fa-twitter"></i> mr_ubik</a>
</footer>
